\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{blindtext}
\usepackage{hyperref}
\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{note}{Note}
\begin{document}
\setcounter{section}{0}
\title{Chapter 9 Review Notes}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Introduction to Variational Autoencoder}\\
{\large Group U8: Lake Yin, Zhiqi Wang}\\
Spring 2023
\end{center}
\section{Motivation: From Autoencoder to Variational Autoencoder}
\subsection{Review on Autoencoder}
Autoencoder is a Deep Learning model that encode high dimensional information into latent space (encoder) to learn a lower dimension representation while ignoring the noise. 

In assignment 4, we've worked with a simple autoencoder model that encode the FashionMNIST dataset into 1 hidden layer and then reconstruct it to a output layer, which is the reconstructed image.

We could think about the latent space representation as features of an image, if we just use the generator of the autoencoder to generate a new image with the latent space representation, we could get a new image that is similar to the original image.

\section{Model of Variational Autoencoder}

\subsection{VAE structure From Autoencoder}
\begin{figure}[h]
    \begin{center}
\includegraphics*{vae-graph.png}
    \end{center}
\caption{VAE structure}
\end{figure}


For VAE to work, we need to make some assumption about the distribution of the latent space variable. We assume that the dataset $X$ is generated by some random process involving a unknown random variable $z$. This $z$ is generated from some prior distribution $p_{\theta^*}(z)$, and the data point $x \in X$ is generated from some conditional distribution $p_{\theta^*}(x|z)$. 

As the graph suggests above, we have a encoder that maps the data point $x$ to the latent space variable $z$, and a decoder that maps the latent space variable $z$ to the data point $\hat{x}$. What's different of this graph comparing to a classic autoencoder is that we have a prior distribution $p_{\theta^*}(z)$. A common choice for the prior distribution is a standard normal distribution. As the graph suggests, we have a mean vector and a standard deviation vector that maps the latent space variable $z$ to the mean and standard deviation of the distribution.

\begin{note}
The encoder is often called the inference model, and the decoder is often called the generative model. 

\begin{center} The encoder computes $q_\phi(z|x)$\\The decoder computes $p_\theta(x|z)$ \end{center}
\end{note}

\subsection{VAE's objective function}

There are two part of the objective function of VAE, the reconstruction loss and the KL divergence loss. The generative model will be improved by approximately minimizing the marginal likelihood. The inference model will be improved by minimizing the KL divergence between the approximate posterior $q_\phi(z|x)$ and the prior $p_{\theta^*}(z)$.

Therefore, we could write the objective function as:


\begin{align}
\phi^*, \theta^* &= \text{argmax}_{\phi} \log p_\theta(x|z) - \text{argmin}_{\phi, \theta}D_\text{KL}(q_\phi(z|x)||p_{\theta^*}(z))
\end{align}

Where $\phi^*$ is the variational parameters, $\theta^*$ is the parameters of the generative model.

\begin{shaded}
Interpretation of RHS of equation (1):
    \begin{center}
    $\text{argmax}_{\phi} \log p_\theta(x|z)$: We are finding the $\phi$ that maximize the marginal likelihood. This is improving our generative model

    $\text{argmin}_{\phi, \theta}D_\text{KL}(q_\phi(z|x)||p_{\theta^*}(z))$: We are finding the $\phi$ and $\theta$ that minimize the KL divergence between the approximate posterior $q_\phi(z|x)$ and the prior $p_{\theta^*}(z)$. This is improving our inference model
    \end{center}
\end{shaded}

Continue on the objective function as equation (1):

Define a new term ELBO (Evidence Lower Bound) as: 
\begin{align}
    \mathcal{L}_{\phi, \theta}(x) &= \log p_\theta(x|z) - D_\text{KL}(q_\phi(z|x)||p_{\theta^*}(z))\\
    &= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)]\\
    &= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z) - \log q_\phi(z|x)]
\end{align}


\section{SGD and Reparameterization Trick}

\subsection{SGD}

The objective function of VAE is a non-convex function, so we need to use SGD to optimize the objective function. The objective function, when training with a batch of data, is:

\begin{align}
    \mathcal{L}_{\phi, \theta}(D) = \frac{1}{|D|}\sum_{x \in D} \mathcal{L}_{\phi, \theta}(x)
\end{align}

And the gradient of the objective function w.r.t. the generative model parameters $\theta$ is:
\begin{align}
    \nabla_\theta \mathcal{L}_{\phi, \theta}(x) &= \nabla_\theta\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z) - \log q_\phi(z|x)]\\
    &= \mathbb{E}_{q_\phi(z|x)}[\nabla_\theta\log p_\theta(x|z) - \nabla_\theta\log q_\phi(z|x)]\\
    &\approx \nabla_\theta(\log p_\theta(x|z) - \log q_\phi(z|x))\\
    &= \nabla_\theta\log p_\theta(x|z)
\end{align}

And the gradient of the objective function w.r.t. the variational parameters $\phi$ is harder to approach, since:
\begin{align}
    \nabla_\phi \mathcal{L}_{\phi, \theta}(x) &= \nabla_\phi\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z) - \log q_\phi(z|x)]\\
    &\neq \mathbb{E}_{q_\phi(z|x)}[\nabla_\phi\log p_\theta(x|z) - \nabla_\phi\log q_\phi(z|x)]
\end{align}

This is because the gradient of the objective function w.r.t. the variational parameters $\phi$ is a stochastic gradient, and we can't use the stochastic gradient descent to optimize the objective function in this form, since the expectation is taken over the variational parameters $\phi$.

\subsection{Reparameterization Trick}

To solve the problem of the stochastic gradient, we can express the random variable $z$ as a deterministic function of the variational parameters $\phi$, the data point $x$, and a random variable $\epsilon$ that is sampled from a prior distribution $p(\epsilon)$:
\begin{align}
    z=g_\phi(x, \epsilon, \phi)
\end{align}
This is called the reparameterization trick, and the $\epsilon$ is usually sampled from a standard normal distribution $p(\epsilon) \sim \mathcal{N}(0, 1)$.

\newpage
\subsection{Gradient of Expectation with Reparameterization Trick}

Given the change of variable $z=g_\phi(x, \epsilon, \phi)$, we can rewrite the gradient of the objective function w.r.t. the variational parameters $\phi$ as:

\begin{align}
    \mathbb{E}_{q_\phi(z|x)}[f(z)] = \mathbb{E}_{p(\epsilon)}[f(g(x, \epsilon, \phi))] = \mathbb{E}_{p(\epsilon)}[f(z)]
\end{align}

Then we could take gradient of equation (13) w.r.t. $\phi$:
\begin{align}
    \nabla_\phi \mathbb{E}_{q_\phi(z|x)}[f(z)] &= \nabla_\phi \mathbb{E}_{p(\epsilon)}[f(z)]\\
    &= \mathbb{E}_{p(\epsilon)}[\nabla_\phi f(z)]\\
    &\approx \nabla_\phi f(z)
\end{align}

\begin{figure}[h]
\includegraphics*[width=\textwidth]{reparam-graph.png}
\caption{Reparameterization trick demonstration: on the left is the original form (without) reparameterization trick, and on the right is the reparameterization trick form. As it can be seen, the reparameterization trick enables us to take gradient of the expectation w.r.t. the variational parameters $\phi$, therefore we can backprogagate the gradient to the generative model parameters $\theta$.}
\end{figure}

\subsection{Gradient of Objective Function with Reparameterization Trick}

Now we can take gradient of the objective function w.r.t. the variational parameters $\phi$:
\begin{align}
    \mathcal{L}_{\theta, \phi}(x) &= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z) - \log q_\phi(z|x)]\\
    &= \mathbb{E}_{p(\epsilon)}[\log p_\theta(x|z) - \log q_\phi(z|x)]\\
\end{align}

From equation (19), we can form our ELBO in with the reparameterized parameter $z$:

\begin{align}
    \epsilon &\sim p(\epsilon)\\
    z &= g_\phi(x, \epsilon, \phi)\\
    \mathcal{\tilde{L}}_{\theta, \phi}(x) &= \log p_\theta(x|z) - \log q_\phi(z|x)
\end{align}

$\mathcal{\tilde{L}}_{\theta, \phi}(x)$ is a Monte Carlo estimator of the objective ELBO of a single data point $x$ now we could use it for SGD.

\section{Further Reading}

This lecture note is based on the following papers:

\begin{itemize}
    \item \href{https://arxiv.org/abs/1312.6114}{Auto-Encoding Variational Bayes}
    \item \href{https://arxiv.org/abs/1906.02691}{An Introduction to Variational Autoencoders}
\end{itemize}

We started from autoencoder to variational autoencoder, then we defined the objective based on the encoder-decoder structure, and then we derived the reparameterization trick. The original paper of VAE is a bit hard to understand since it started with the motivation from probability, so I hope this lecture note can help you understand the VAE better.

\end{document}